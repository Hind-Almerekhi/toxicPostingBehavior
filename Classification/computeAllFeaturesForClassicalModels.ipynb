{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a8dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import pickle \n",
    "import mglearn\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "import enchant\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "d = enchant.Dict(\"en_US\")\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict  # For word frequency\n",
    "import spacy  # For preprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from time import time  # To time our operations\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "SEED=13\n",
    "DIMS=300\n",
    "FEAT=3000\n",
    "#python -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
    "#File locations\n",
    "\n",
    "root_dir = os.path.abspath(os.curdir)\n",
    "base_dir = os.path.dirname(root_dir)+\"/Data/\"\n",
    "\n",
    "#Create directory to save features\n",
    "feat_dir = os.path.dirname(root_dir)+\"/Features/\"\n",
    "os.makedirs(feat_dir, exist_ok=True)\n",
    "bad = base_dir+'en'\n",
    "training_dataset = base_dir+'trainingDataset-toxicComments.csv'\n",
    "#For discourse we will use this lexicon\n",
    "#https://github.com/discourse-lab/en_dimlex\n",
    "discourse = base_dir+'en_dimlex-parsed.txt'\n",
    "#Positive words are taken from the following resource \n",
    "#https://github.com/zengyan-97/Sentiment-Lexicon\n",
    "positive = base_dir+'positive.txt'\n",
    "negative = base_dir+'negative.txt'\n",
    "disList = [x.strip() for x in open(discourse)]\n",
    "posList = [x.strip() for x in open(positive)]\n",
    "negList = [x.strip() for x in open(negative)]\n",
    "proList = [x.strip() for x in open(bad)]\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will', 'would', 'should']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053c1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean the dataset\n",
    "#credit goes to :https://github.com/LoLei/redditcleaner\n",
    "def clean(text, newline=True, quote=True, bullet_point=True, \n",
    "          link=True, strikethrough=True, spoiler=True,\n",
    "          code=True, superscript=True, table=True, heading=True):\n",
    "    \"\"\"\n",
    "    Cleans text (string).\n",
    "    Removes common Reddit special characters/symbols:\n",
    "      * \\n (newlines)\n",
    "      * &gt; (> quotes)\n",
    "      * * or &amp;#x200B; (bullet points)\n",
    "      * []() (links)\n",
    "      * etc (see below)\n",
    "    Specific removals can be turned off, but everything is on by default.\n",
    "    Standard punctuation etc is deliberately not removed, can be done in a\n",
    "    second round manually, or may be preserved in any case.\n",
    "    \"\"\"\n",
    "    # Newlines (replaced with space to preserve cases like word1\\nword2)\n",
    "    if newline:\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "        # Remove resulting ' '\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # > Quotes\n",
    "    if quote:\n",
    "        text = re.sub(r'\\\"?\\\\?&?gt;?', '', text)\n",
    "\n",
    "    # Bullet points/asterisk (bold/italic)\n",
    "    if bullet_point:\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # []() Link (Also removes the hyperlink)\n",
    "    if link:\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text = re.sub('~', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text = re.sub('&lt;', '', text)\n",
    "        text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Code, inline and block\n",
    "    if code:\n",
    "        text = re.sub('`', '', text)\n",
    "\n",
    "    # Superscript (Preserves the text)\n",
    "    if superscript:\n",
    "        text = re.sub(r'\\^\\((.*?)\\)', r'\\1', text)\n",
    "\n",
    "    # Table\n",
    "    if table:\n",
    "        text = re.sub(r'\\|', ' ', text)\n",
    "        text = re.sub(':-', '', text)\n",
    "\n",
    "    # Heading\n",
    "    if heading:\n",
    "        text = re.sub('#', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def cleanTokens(text): \n",
    "    stemmer = WordNetLemmatizer()\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(text))\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    return document\n",
    "\n",
    "\n",
    "def cleanVecs(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc68f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read, clean, and fix the dataset, then split it to training and testing\n",
    "def generateData(datasetFlag,subFlag):\n",
    "    train = pd.read_csv(training_dataset)\n",
    "    train['comment_raw'] = train['comment_text']\n",
    "    train['comment_text'] = train['comment_text'].apply(lambda x: clean(x))\n",
    "    s = train[['highly_toxic','slightly_toxic','non_toxic']]\n",
    "    train['category'] = pd.get_dummies(s).idxmax(1)\n",
    "    train['label'] =train['category'].map({'non_toxic':0, 'slightly_toxic':1,'highly_toxic':2})\n",
    "    train = train.sample(frac=1,random_state=SEED)\n",
    "    print(\"Original shape before preprocessing:\",train.shape)\n",
    "\n",
    "    train['brief_cleaning'] = train['comment_text'].map(lambda x: re.sub(\"[^A-Za-z']+\", ' ', str(x)).lower()) \n",
    "    t = time()\n",
    "    txt = [cleanVecs(doc) for doc in nlp.pipe(train.brief_cleaning.tolist(), batch_size=5000, n_process=-1)]\n",
    "\n",
    "    print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    train['clean'] = txt\n",
    "    # df_clean = pd.DataFrame({'clean': txt})\n",
    "    train = train.dropna()\n",
    "    train = train.drop_duplicates(subset='clean', keep='first')\n",
    "    train = train.reset_index(drop=True)\n",
    "    print(train['comment_text'].head())\n",
    "    print(\"Shape after preprocessing:\",train.shape)\n",
    "    \n",
    "    if datasetFlag == 1:\n",
    "        print(\"Yielding data for BoW\")\n",
    "        train['comment_text'] = train['comment_text'].map(lambda x: cleanTokens(x))\n",
    "        x = train['comment_text']\n",
    "                \n",
    "    elif datasetFlag == 2:\n",
    "        x = train\n",
    "        if subFlag==1:\n",
    "            print(\"Yielding data for W2V\")  \n",
    "        elif subFlag==2:\n",
    "            print(\"Yielding data for D2V\")\n",
    "    \n",
    "    elif datasetFlag == 3:\n",
    "        print(\"Yielding data for Hateful features\")\n",
    "        x = train['comment_raw']\n",
    "    \n",
    "    y = train['label']\n",
    "    printNumbers(y)\n",
    "    y = train['label'].to_numpy()\n",
    "    y = y.reshape(-1,1)\n",
    "    with open(feat_dir+'target.pkl','wb') as f: pickle.dump(y, f)\n",
    "    return x, y\n",
    "      \n",
    "    \n",
    "def printNumbers(y):\n",
    "    nontox, slightly, highly = np.bincount(y)\n",
    "    total = nontox + slightly + highly\n",
    "    print('Total: {}\\n    Non toxic: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, nontox, 100 * nontox / total))\n",
    "    print('Total: {}\\n    Slightly toxic: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, slightly, 100 * slightly / total))\n",
    "    print('Total: {}\\n    Highly toxic: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, highly, 100 * highly / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c446a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word vectroizer instance, then fit it on training and testing splits\n",
    "#Unigram\n",
    "def generateBOWFeats(datasetFlag):\n",
    "    subFlag = 0\n",
    "    x, y = generateData(datasetFlag,subFlag)\n",
    "    tf_vectorizer = CountVectorizer(\n",
    "        stop_words=None,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\w{2,}', #accept tokens that have 1 or more characters\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "        min_df=1,\n",
    "        max_features=FEAT)\n",
    "\n",
    "    # fit data\n",
    "    tf = tf_vectorizer.fit_transform(x).toarray()\n",
    "    with open(feat_dir+'tf.pkl','wb') as f: pickle.dump(tf, f)\n",
    "    with open(feat_dir+'tf.pkl','rb') as f: arrayname1 = pickle.load(f)\n",
    "    print(np.array_equal(tf,arrayname1)) #sanity check\n",
    "\n",
    "    #Bigram\n",
    "    tfbig_vectorizer = CountVectorizer(\n",
    "        stop_words=None,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\w{2,}', #accept tokens that have 1 or more characters\n",
    "        analyzer='word',\n",
    "        ngram_range=(2, 2),\n",
    "        min_df=1,\n",
    "        max_features=FEAT)\n",
    "\n",
    "    # fit data\n",
    "    tfbig = tfbig_vectorizer.fit_transform(x).toarray()\n",
    "    with open(feat_dir+'tfbig.pkl','wb') as f: pickle.dump(tfbig, f)\n",
    "    with open(feat_dir+'tfbig.pkl','rb') as f: arrayname1 = pickle.load(f)\n",
    "    print(np.array_equal(tfbig,arrayname1)) #sanity check\n",
    "\n",
    "    #Ngram\n",
    "    tfn_vectorizer = CountVectorizer(\n",
    "        stop_words=None,\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\w{2,}', #accept tokens that have 1 or more characters\n",
    "        analyzer='word',\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=1,\n",
    "        max_features=FEAT)\n",
    "\n",
    "    # fit data\n",
    "    tfn = tfn_vectorizer.fit_transform(x).toarray()  \n",
    "    with open(feat_dir+'tfn.pkl','wb') as f: pickle.dump(tfn, f)\n",
    "    with open(feat_dir+'tfn.pkl','rb') as f: arrayname1 = pickle.load(f)\n",
    "    np.array_equal(tfn,arrayname1) #sanity check\n",
    "\n",
    "    #TFIDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        stop_words=None,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "        ngram_range=(1, 1),\n",
    "        max_features=FEAT)\n",
    "\n",
    "    # fit data\n",
    "    tfidf = tfidf_vectorizer.fit_transform(x).toarray()\n",
    "    with open(feat_dir+'tfidf.pkl','wb') as f: pickle.dump(tfidf, f)\n",
    "    with open(feat_dir+'tfidf.pkl','rb') as f: arrayname1 = pickle.load(f)\n",
    "    print(np.array_equal(tfidf,arrayname1)) #sanity check\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c6a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute w2v and d2v features\n",
    "def generateGensimFeats(datasetFlag, subFlag):\n",
    "    if subFlag == 1:\n",
    "        x, y = generateData(datasetFlag,subFlag)\n",
    "        train = pd.DataFrame()\n",
    "        train = x\n",
    "        sent = [row.split() for row in train['clean']]\n",
    "        print(len(sent))\n",
    "        phrases = Phrases(sent, min_count=2, progress_per=100)\n",
    "        bigram = Phraser(phrases)\n",
    "        sentences = bigram[sent]\n",
    "        com_tokens = train['clean'].apply(lambda x: x.split()) # tokenizing \n",
    "        w2v_model = Word2Vec(min_count=2,\n",
    "                             window=10,\n",
    "                             vector_size=DIMS,\n",
    "                             hs=1,\n",
    "                             sg=1,\n",
    "                             negative=10,\n",
    "                             seed = SEED,\n",
    "                             workers=cores-1)\n",
    "        print(\"Building w2v vocabulary\")\n",
    "        w2v_model.build_vocab(sentences, progress_per=1000)\n",
    "        #print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        name = feat_dir+\"word2vec-model.bin\"\n",
    "        if os.path.isfile(name):\n",
    "            print(\"Model exists, no need to train\")\n",
    "            print(\"The model will be leaded\")\n",
    "            w2v_model = Word2Vec.load(name)\n",
    "            print(w2v_model.wv.most_similar(positive=[\"fuck\"]))\n",
    "            print(len(w2v_model.wv['fuck']))\n",
    "            print(\"Vocab size:\",len(w2v_model.wv))\n",
    "        else:\n",
    "            print(\"There is no model. The model will be trained now:\")    \n",
    "            t = time()\n",
    "            w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=15, report_delay=1)\n",
    "            print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "            #saving and loading the model\n",
    "            w2v_model.save(name)\n",
    "            print(w2v_model.wv.most_similar(positive=[\"fuck\"]))\n",
    "            print(len(w2v_model.wv['fuck']))\n",
    "            print(\"Vocab size:\",len(w2v_model.wv))\n",
    "\n",
    "\n",
    "        def word_vector(tokens, size):\n",
    "            vec = np.zeros(size).reshape((1, size))\n",
    "            count = 0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    vec +=w2v_model.wv[word].reshape((1, size))\n",
    "                    count += 1.\n",
    "                except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                    continue\n",
    "            if count != 0:\n",
    "                vec /= count\n",
    "            return vec\n",
    "\n",
    "        wordvec_arrays = np.zeros((len(y), DIMS)) \n",
    "        \n",
    "        for i in range(len(y)):  \n",
    "            wordvec_arrays[i,:] = word_vector(com_tokens[i], DIMS)    \n",
    "        wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "        print(\"Trained vectors shape:\",wordvec_df.shape)\n",
    "        name = feat_dir+\"word2vec-features.pkl\"\n",
    "        wordvec_df.to_pickle(name)\n",
    "        del wordvec_df\n",
    "        del wordvec_arrays\n",
    "        gc.collect()\n",
    "        \n",
    "    elif subFlag == 2:\n",
    "        x, y = generateData(datasetFlag,subFlag)\n",
    "        # #Prepare input for doc2vec model\n",
    "        train = pd.DataFrame()\n",
    "        train = x\n",
    "        com_tokens = train['clean'].apply(lambda x: x.split()) # tokenizing \n",
    "        doc_df = pd.DataFrame(com_tokens,columns=['clean'])\n",
    "        doc_df['id'] = train['id']\n",
    "        doc_df['label'] = train['label']\n",
    "\n",
    "        #Prepare doc2vector data\n",
    "        labeled_coms = doc_df.apply(lambda x: TaggedDocument(words=x.clean, tags=[x.id]), axis=1)\n",
    "\n",
    "        #Building doc2vector model\n",
    "        d2v_model=Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model\n",
    "                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n",
    "                  vector_size=DIMS, # no. of desired features\n",
    "                  window=15, # width of the context window                                  \n",
    "                  negative=7, # if > 0 then negative sampling will be used\n",
    "                  min_count=2, # Ignores all words with total frequency lower than 5.                                  \n",
    "                  workers=cores-1, # no. of cores                                  \n",
    "                  alpha=0.03, # learning rate   \n",
    "        #           alpha=0.0025, \n",
    "        #           min_alpha=0.000001, \n",
    "                  seed = SEED # for reproducibility\n",
    "                 ) \n",
    "        print(\"Building D2V vocabulary\")\n",
    "        d2v_model.build_vocab([i for i in tqdm(labeled_coms)])\n",
    "        #print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        \n",
    "        name = feat_dir+\"doc2vec-model.bin\"\n",
    "        if os.path.isfile(name):\n",
    "            print(\"Model exists, no need to train\")\n",
    "            print(\"The model will be leaded\")\n",
    "            d2v_model = Doc2Vec.load(name)\n",
    "            print(d2v_model.dv['delfqkc'])\n",
    "            print(len(d2v_model.dv['delfqkc']))\n",
    "            print(\"Vocab size:\",len(d2v_model.wv))\n",
    "        else:\n",
    "            print(\"There is no model. The model will be trained now:\")    \n",
    "            t = time()\n",
    "            d2v_model.train(labeled_coms, total_examples=d2v_model.corpus_count, epochs=15, report_delay=1)\n",
    "            print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "            #saving and loading the model\n",
    "            d2v_model.save(name)\n",
    "            print(d2v_model.dv['delfqkc'])\n",
    "            print(len(d2v_model.dv['delfqkc']))\n",
    "            print(\"Vocab size:\",len(d2v_model.wv))\n",
    "\n",
    "        docvec_arrays = np.zeros((len(com_tokens), DIMS)) \n",
    "        for i in range(len(train)):\n",
    "            docvec_arrays[i,:] = d2v_model.dv[i].reshape((1,DIMS))    \n",
    "\n",
    "        docvec_df = pd.DataFrame(docvec_arrays) \n",
    "        docvec_df.shape\n",
    "        print(\"Trained vectors shape:\",docvec_df.shape)     \n",
    "        name = feat_dir+\"doc2vec-features.pkl\"\n",
    "        docvec_df.to_pickle(name)\n",
    "        \n",
    "        del docvec_df\n",
    "        del docvec_arrays\n",
    "        gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c00ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute hateful features\n",
    "def count_repeated(text):\n",
    "    repeated_threshold = 1\n",
    "    text_splitted = text.split()\n",
    "    word_counts = Counter([x for x in text if x in string.punctuation])\n",
    "    countPunkts = [item for item in list(word_counts.values()) if item > repeated_threshold]\n",
    "    return len(countPunkts)\n",
    "\n",
    "def count_unknown(text):\n",
    "    spellCounter = 0\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    for ele in text:\n",
    "        if ele in string.punctuation:\n",
    "            text = text.replace(ele, \"\")\n",
    "    #Number of unknown words\n",
    "    for word in text.strip().split():\n",
    "        if d.check(word) is False:\n",
    "            #print(word)\n",
    "            spellCounter += 1\n",
    "    return spellCounter\n",
    "\n",
    "def tag_part_of_speech(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    pos_list = pos_tag(text_splited)\n",
    "    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    return[noun_count, adjective_count, verb_count]\n",
    "\n",
    "def generateHateFeats(datasetFlag):\n",
    "    subFlag = 0\n",
    "    x, y = generateData(datasetFlag,subFlag)\n",
    "    train = pd.DataFrame()\n",
    "    train['comment_text'] = x\n",
    "    train['total_length'] = train['comment_text'].apply(len)\n",
    "    train['averageWords'] = train['comment_text'].apply(lambda comment: sum(len(word) for word in comment.split())/len(comment.split()))\n",
    "    train['words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    train['words_vs_length'] = train['words'] / train['total_length']\n",
    "\n",
    "    train['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    train['capitals_vs_length'] = train['capitals'] / train['total_length']\n",
    "    train['capitals_vs_words'] = train['capitals'] / train['words']\n",
    "\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    train['stopwords'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in eng_stopwords))\n",
    "    train['stopwords_vs_length'] = train['stopwords'] / train['total_length']\n",
    "    train['stopwords_vs_words'] = train['stopwords'] / train['words']\n",
    "\n",
    "    train['punctuation'] = train['comment_text'].apply(\n",
    "        lambda comment: sum(comment.count(w) for w in string.punctuation))\n",
    "    train['punctuation_vs_length'] = train['punctuation'] / train['total_length']\n",
    "    train['punctuation_vs_words'] = train['punctuation'] / train['words']\n",
    "    train['period'] = train['comment_text'].apply(lambda comment: comment.count('.'))\n",
    "   \n",
    "    train['quote'] = train['comment_text'].apply(lambda comment: len([w for w in comment if w=='\"' or w==\"'\"]))\n",
    "    train['unique_words'] = train['comment_text'].apply(\n",
    "        lambda comment: len(set(w for w in comment.split())))\n",
    "    train['unique_words_vs_length'] = train['unique_words'] / train['total_length']\n",
    "    train['unique_words_vs_words'] = train['unique_words'] / train['words']\n",
    "\n",
    "    train['nonAlpha'] = train['comment_text'].str.count(r'[^a-zA-Z0-9 ]')\n",
    "\n",
    "    train['repeatedPunct'] = train['comment_text'].apply(lambda comment: count_repeated(comment))\n",
    "\n",
    "    train['discourse'] = train['comment_text'].apply(lambda comment:len([w.lower() for w in comment.split() if w in disList]))\n",
    "\n",
    "    train['politeness'] = train['comment_text'].apply(lambda comment:len([w.lower() for w in comment.split() if w in posList]))\n",
    "\n",
    "    train['rudeness'] = train['comment_text'].apply(lambda comment:len([w.lower() for w in comment.split() if w in negList]))\n",
    "    train['singleTokens'] = train['comment_text'].apply(lambda comment:len([w for w in comment.split() if len(w)==1]))\n",
    "    train['modCount'] = train['comment_text'].apply(lambda comment:len([w.lower() for w in comment.split() if w in modals]))    \n",
    "    train['unknownWords'] = train['comment_text'].apply(lambda comment: count_unknown(comment))\n",
    "\n",
    "    train['profaneCount'] = train['comment_text'].apply(lambda comment:len([w.lower() for w in comment.split() if w in bad]))\n",
    "    train['profaneRatio'] = train['profaneCount'] / train['words']  \n",
    "\n",
    "    train['nouns'], train['adjectives'], train['verbs'] = zip(*train['comment_text'].apply(\n",
    "        lambda comment: tag_part_of_speech(comment)))\n",
    "\n",
    "    train['nouns_vs_length'] = train['nouns'] / train['total_length']\n",
    "    train['adjectives_vs_length'] = train['adjectives'] / train['total_length']\n",
    "    train['verbs_vs_length'] = train['verbs'] / train['total_length']\n",
    "    train['nouns_vs_words'] = train['nouns'] / train['words']\n",
    "    train['adjectives_vs_words'] = train['adjectives'] / train['words']\n",
    "    train['verbs_vs_words'] = train['verbs'] / train['words']\n",
    "    train = train.drop('comment_text', 1)\n",
    "    print(train.head())\n",
    "    name = feat_dir+\"hatefulFeats.pkl\"\n",
    "    train.to_pickle(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31883cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineAllFeatures():\n",
    "    featureFiles = {'tf.pkl':'Unigram','tfbig.pkl':'Bigram','tfn.pkl':'Ngram','tfidf.pkl':'TFIDF',\\\n",
    "        'hatefulFeats.pkl':'Hateful','word2vec-features.pkl':'Word2Vec','doc2vec-features.pkl':'Doc2Vec'}\n",
    "    lst = []\n",
    "    for filename, title in featureFiles.items():   \n",
    "        with open(feat_dir+filename,'rb') as f: train = pickle.load(f)\n",
    "        print(train.shape)\n",
    "        lst.append(train)\n",
    "  \n",
    "    result = np.hstack(lst)\n",
    "    print(result.shape)\n",
    "    name = feat_dir+\"allFeatures.pkl\"\n",
    "    with open(name,'wb') as f: pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4040999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape before preprocessing: (10083, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:52:38: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.59 mins\n",
      "0    With my SO (30 years together), this kind of q...\n",
      "1    I feel like this is a sign of how young Americ...\n",
      "2    I was once interviewed as a character witness ...\n",
      "3    RJ Reynolds tobacco (Winston's, and Camel), Kr...\n",
      "4    Lol, so you didn't take the transfer that's li...\n",
      "Name: comment_text, dtype: object\n",
      "Shape after preprocessing: (10065, 10)\n",
      "Yielding data for BoW\n",
      "Total: 10065\n",
      "    Non toxic: 8210 (81.57% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Slightly toxic: 1189 (11.81% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Highly toxic: 666 (6.62% of total)\n",
      "\n",
      "True\n",
      "True\n",
      "True\n",
      "Original shape before preprocessing: (10083, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:30: collecting all words and their counts\n",
      "INFO - 02:53:30: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #100, processed 2519 words and 3628 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #200, processed 5961 words and 7935 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #300, processed 8874 words and 11292 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #400, processed 11689 words and 14457 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #500, processed 14742 words and 17738 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #600, processed 17588 words and 20653 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #700, processed 20656 words and 23942 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #800, processed 24018 words and 27375 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #900, processed 26737 words and 30122 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1000, processed 29445 words and 32759 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1100, processed 32767 words and 36121 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1200, processed 36030 words and 39242 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1300, processed 39364 words and 42332 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1400, processed 42356 words and 45229 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1500, processed 45150 words and 47791 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1600, processed 48023 words and 50394 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1700, processed 51452 words and 53549 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1800, processed 54504 words and 56251 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #1900, processed 57728 words and 59157 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2000, processed 60386 words and 61563 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2100, processed 63135 words and 64048 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2200, processed 66040 words and 66557 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2300, processed 69103 words and 69234 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2400, processed 73026 words and 72717 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2500, processed 75900 words and 75282 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2600, processed 78691 words and 77724 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2700, processed 81435 words and 80104 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.57 mins\n",
      "0    With my SO (30 years together), this kind of q...\n",
      "1    I feel like this is a sign of how young Americ...\n",
      "2    I was once interviewed as a character witness ...\n",
      "3    RJ Reynolds tobacco (Winston's, and Camel), Kr...\n",
      "4    Lol, so you didn't take the transfer that's li...\n",
      "Name: comment_text, dtype: object\n",
      "Shape after preprocessing: (10065, 10)\n",
      "Yielding data for W2V\n",
      "Total: 10065\n",
      "    Non toxic: 8210 (81.57% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Slightly toxic: 1189 (11.81% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Highly toxic: 666 (6.62% of total)\n",
      "\n",
      "10065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:30: PROGRESS: at sentence #2800, processed 84492 words and 82765 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #2900, processed 87344 words and 85253 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3000, processed 90063 words and 87601 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3100, processed 92890 words and 90033 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3200, processed 95738 words and 92511 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3300, processed 98632 words and 94942 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3400, processed 101014 words and 96923 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3500, processed 104033 words and 99437 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3600, processed 107006 words and 101892 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3700, processed 109945 words and 104481 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3800, processed 113258 words and 107223 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #3900, processed 116291 words and 109762 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4000, processed 119158 words and 112139 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4100, processed 122248 words and 114761 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4200, processed 125405 words and 117283 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4300, processed 128260 words and 119640 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4400, processed 131134 words and 122008 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4500, processed 133949 words and 124304 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4600, processed 137268 words and 126930 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4700, processed 139951 words and 129018 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4800, processed 142440 words and 130992 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #4900, processed 145542 words and 133417 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5000, processed 148629 words and 135900 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5100, processed 151601 words and 138302 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5200, processed 154713 words and 140473 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5300, processed 157962 words and 142965 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5400, processed 160732 words and 145206 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5500, processed 163131 words and 147095 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5600, processed 165792 words and 149143 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5700, processed 168961 words and 151705 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5800, processed 171833 words and 154006 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #5900, processed 174773 words and 156314 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6000, processed 177865 words and 158652 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6100, processed 180566 words and 160682 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6200, processed 183672 words and 163053 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6300, processed 186714 words and 165399 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6400, processed 189593 words and 167658 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6500, processed 192101 words and 169608 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6600, processed 194658 words and 171590 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6700, processed 197428 words and 173600 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6800, processed 200274 words and 175734 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #6900, processed 203292 words and 178060 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7000, processed 206349 words and 180418 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7100, processed 208847 words and 182228 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7200, processed 211649 words and 184325 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7300, processed 214555 words and 186433 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7400, processed 217292 words and 188524 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7500, processed 220582 words and 191050 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7600, processed 223347 words and 193150 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7700, processed 226855 words and 195702 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7800, processed 229720 words and 197834 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #7900, processed 232826 words and 200140 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8000, processed 235768 words and 202258 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8100, processed 238291 words and 204029 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8200, processed 240989 words and 205940 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8300, processed 243961 words and 208128 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8400, processed 246585 words and 210046 word types\n",
      "INFO - 02:53:30: PROGRESS: at sentence #8500, processed 249414 words and 212038 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #8600, processed 251848 words and 213838 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #8700, processed 254379 words and 215675 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #8800, processed 257283 words and 217791 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #8900, processed 259894 words and 219713 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9000, processed 262454 words and 221528 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9100, processed 265120 words and 223433 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9200, processed 268012 words and 225543 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9300, processed 271628 words and 228162 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9400, processed 274210 words and 229997 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9500, processed 276997 words and 232040 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9600, processed 279644 words and 233940 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9700, processed 282973 words and 236341 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9800, processed 286033 words and 238467 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9900, processed 288987 words and 240621 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #10000, processed 292213 words and 242986 word types\n",
      "INFO - 02:53:31: collected 244289 token types (unigram + bigrams) from a corpus of 294060 words and 10065 sentences\n",
      "INFO - 02:53:31: merged Phrases<244289 vocab, min_count=2, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 02:53:31: Phrases lifecycle event {'msg': 'built Phrases<244289 vocab, min_count=2, threshold=10.0, max_vocab_size=40000000> in 0.44s', 'datetime': '2022-04-30T02:53:31.062132', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO - 02:53:31: exporting phrases from Phrases<244289 vocab, min_count=2, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 02:53:31: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<2512 phrases, min_count=2, threshold=10.0> from Phrases<244289 vocab, min_count=2, threshold=10.0, max_vocab_size=40000000> in 0.38s', 'datetime': '2022-04-30T02:53:31.441630', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO - 02:53:31: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.025)', 'datetime': '2022-04-30T02:53:31.465563', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "INFO - 02:53:31: collecting all words and their counts\n",
      "INFO - 02:53:31: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #1000, processed 27947 words, keeping 7100 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:31: PROGRESS: at sentence #2000, processed 57225 words, keeping 10652 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #3000, processed 85397 words, keeping 13256 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #4000, processed 112979 words, keeping 15379 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #5000, processed 140987 words, keeping 17160 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #6000, processed 168517 words, keeping 18782 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #7000, processed 195622 words, keeping 20075 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building w2v vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:31: PROGRESS: at sentence #8000, processed 223597 words, keeping 21337 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #9000, processed 248880 words, keeping 22374 word types\n",
      "INFO - 02:53:31: PROGRESS: at sentence #10000, processed 277142 words, keeping 23474 word types\n",
      "INFO - 02:53:31: collected 23540 word types from a corpus of 278894 raw words and 10065 sentences\n",
      "INFO - 02:53:31: Creating a fresh vocabulary\n",
      "INFO - 02:53:31: Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 13913 unique words (59.103653355989806%% of original 23540, drops 9627)', 'datetime': '2022-04-30T02:53:31.783172', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 02:53:31: Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 269267 word corpus (96.54815091038172%% of original 278894, drops 9627)', 'datetime': '2022-04-30T02:53:31.784169', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 02:53:31: deleting the raw counts dictionary of 23540 items\n",
      "INFO - 02:53:31: sample=0.001 downsamples 32 most-common words\n",
      "INFO - 02:53:31: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 254229.25904130668 word corpus (94.4%% of prior 269267)', 'datetime': '2022-04-30T02:53:31.851989', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 02:53:31: constructing a huffman tree from 13913 words\n",
      "INFO - 02:53:32: built huffman tree with maximum node depth 17\n",
      "INFO - 02:53:32: estimated required memory for 13913 words and 300 dimensions: 59825900 bytes\n",
      "INFO - 02:53:32: resetting layer weights\n",
      "INFO - 02:53:32: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-04-30T02:53:32.352678', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "INFO - 02:53:32: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 13913 vocabulary and 300 features, using sg=1 hs=1 sample=0.001 negative=10 window=10 shrink_windows=True', 'datetime': '2022-04-30T02:53:32.353647', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no model. The model will be trained now:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:33: EPOCH 1 - PROGRESS: at 31.43% examples, 74048 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:34: EPOCH 1 - PROGRESS: at 89.62% examples, 107524 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:34: EPOCH - 1 : training on 278894 raw words (254133 effective words) took 2.2s, 118161 effective words/s\n",
      "INFO - 02:53:35: EPOCH 2 - PROGRESS: at 31.43% examples, 79865 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:36: EPOCH 2 - PROGRESS: at 85.72% examples, 103831 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:36: EPOCH - 2 : training on 278894 raw words (254266 effective words) took 2.2s, 115719 effective words/s\n",
      "INFO - 02:53:37: EPOCH 3 - PROGRESS: at 31.43% examples, 73871 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:38: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:38: EPOCH 3 - PROGRESS: at 81.81% examples, 99020 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 02:53:38: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:39: EPOCH - 3 : training on 278894 raw words (254128 effective words) took 2.2s, 115161 effective words/s\n",
      "INFO - 02:53:40: EPOCH 4 - PROGRESS: at 31.43% examples, 62809 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:41: EPOCH 4 - PROGRESS: at 100.00% examples, 108593 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 02:53:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:41: EPOCH - 4 : training on 278894 raw words (254316 effective words) took 2.3s, 108551 effective words/s\n",
      "INFO - 02:53:42: EPOCH 5 - PROGRESS: at 35.08% examples, 78901 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:43: EPOCH - 5 : training on 278894 raw words (254196 effective words) took 2.1s, 118882 effective words/s\n",
      "INFO - 02:53:44: EPOCH 6 - PROGRESS: at 35.08% examples, 75883 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:45: EPOCH 6 - PROGRESS: at 85.72% examples, 98000 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:45: EPOCH - 6 : training on 278894 raw words (254243 effective words) took 2.4s, 107877 effective words/s\n",
      "INFO - 02:53:47: EPOCH 7 - PROGRESS: at 27.63% examples, 62777 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:48: EPOCH 7 - PROGRESS: at 78.06% examples, 87984 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:48: EPOCH - 7 : training on 278894 raw words (254252 effective words) took 2.6s, 97316 effective words/s\n",
      "INFO - 02:53:49: EPOCH 8 - PROGRESS: at 27.63% examples, 70791 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:50: EPOCH 8 - PROGRESS: at 78.06% examples, 94221 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 02:53:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:51: EPOCH - 8 : training on 278894 raw words (254203 effective words) took 2.5s, 103156 effective words/s\n",
      "INFO - 02:53:52: EPOCH 9 - PROGRESS: at 31.43% examples, 81332 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:53: EPOCH 9 - PROGRESS: at 85.72% examples, 101640 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:53: EPOCH - 9 : training on 278894 raw words (254251 effective words) took 2.2s, 113362 effective words/s\n",
      "INFO - 02:53:54: EPOCH 10 - PROGRESS: at 31.43% examples, 69893 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:55: EPOCH 10 - PROGRESS: at 81.81% examples, 95423 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:55: EPOCH - 10 : training on 278894 raw words (254298 effective words) took 2.3s, 109921 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:53:56: EPOCH 11 - PROGRESS: at 31.43% examples, 65459 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:53:57: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:53:57: EPOCH 11 - PROGRESS: at 81.81% examples, 91016 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 02:53:57: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:53:57: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:53:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:53:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:53:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:53:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:53:58: EPOCH - 11 : training on 278894 raw words (254384 effective words) took 2.5s, 102841 effective words/s\n",
      "INFO - 02:53:59: EPOCH 12 - PROGRESS: at 31.43% examples, 70179 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:00: EPOCH 12 - PROGRESS: at 85.72% examples, 97443 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:00: EPOCH - 12 : training on 278894 raw words (254338 effective words) took 2.3s, 108660 effective words/s\n",
      "INFO - 02:54:01: EPOCH 13 - PROGRESS: at 27.63% examples, 63829 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 02:54:02: EPOCH 13 - PROGRESS: at 78.06% examples, 91196 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 02:54:02: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:02: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:02: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:02: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:03: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:03: EPOCH - 13 : training on 278894 raw words (254321 effective words) took 2.6s, 99652 effective words/s\n",
      "INFO - 02:54:04: EPOCH 14 - PROGRESS: at 31.43% examples, 73006 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:54:04: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:05: EPOCH 14 - PROGRESS: at 85.72% examples, 102124 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:05: EPOCH - 14 : training on 278894 raw words (254173 effective words) took 2.2s, 116503 effective words/s\n",
      "INFO - 02:54:06: EPOCH 15 - PROGRESS: at 35.08% examples, 81138 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:07: EPOCH 15 - PROGRESS: at 85.70% examples, 102401 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:07: EPOCH - 15 : training on 278894 raw words (254195 effective words) took 2.2s, 116267 effective words/s\n",
      "INFO - 02:54:07: Word2Vec lifecycle event {'msg': 'training on 4183410 raw words (3813697 effective words) took 35.1s, 108636 effective words/s', 'datetime': '2022-04-30T02:54:07.458732', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 02:54:07: Word2Vec lifecycle event {'fname_or_handle': 'C:\\\\Users\\\\Hind\\\\Desktop\\\\GitHub code/Features/word2vec-model.bin', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-30T02:54:07.459730', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "INFO - 02:54:07: not storing attribute cum_table\n",
      "INFO - 02:54:07: saved C:\\Users\\Hind\\Desktop\\GitHub code/Features/word2vec-model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.59 mins\n",
      "[('srsly', 0.4781668484210968), ('negan', 0.40163785219192505), ('rephrase', 0.3981514871120453), ('academy', 0.3854159116744995), ('them_pregnant', 0.38458365201950073), ('scientology', 0.3814558982849121), ('pull_pant', 0.38130810856819153), ('rockstar', 0.3808191418647766), ('precedent', 0.3745284378528595), ('barbie', 0.37427619099617004)]\n",
      "300\n",
      "Vocab size: 13913\n",
      "Trained vectors shape: (10065, 300)\n",
      "Original shape before preprocessing: (10083, 8)\n",
      "Time to clean up everything: 0.57 mins\n",
      "0    With my SO (30 years together), this kind of q...\n",
      "1    I feel like this is a sign of how young Americ...\n",
      "2    I was once interviewed as a character witness ...\n",
      "3    RJ Reynolds tobacco (Winston's, and Camel), Kr...\n",
      "4    Lol, so you didn't take the transfer that's li...\n",
      "Name: comment_text, dtype: object\n",
      "Shape after preprocessing: (10065, 10)\n",
      "Yielding data for D2V\n",
      "Total: 10065\n",
      "    Non toxic: 8210 (81.57% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Slightly toxic: 1189 (11.81% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Highly toxic: 666 (6.62% of total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:54:44: Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d300,n7,w15,mc2,s0.001,t7)', 'datetime': '2022-04-30T02:54:44.022947', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "100%|███████████████████████████████████████████████████████████████████████| 10065/10065 [00:00<00:00, 2018729.43it/s]\n",
      "INFO - 02:54:44: collecting all words and their counts\n",
      "INFO - 02:54:44: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO - 02:54:44: PROGRESS: at example #10000, processed 292213 words (3989087/s), 21101 word types, 10000 tags\n",
      "INFO - 02:54:44: collected 21166 word types and 10065 unique tags from a corpus of 10065 examples and 294060 words\n",
      "INFO - 02:54:44: Creating a fresh vocabulary\n",
      "INFO - 02:54:44: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11630 unique words (54.946612491732026%% of original 21166, drops 9536)', 'datetime': '2022-04-30T02:54:44.185535', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 02:54:44: Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 284524 word corpus (96.75712439638168%% of original 294060, drops 9536)', 'datetime': '2022-04-30T02:54:44.186509', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building D2V vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:54:44: deleting the raw counts dictionary of 21166 items\n",
      "INFO - 02:54:44: sample=0.001 downsamples 33 most-common words\n",
      "INFO - 02:54:44: Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 267843.69758218 word corpus (94.1%% of prior 284524)', 'datetime': '2022-04-30T02:54:44.243357', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 02:54:44: estimated required memory for 11630 words and 300 dimensions: 47818000 bytes\n",
      "INFO - 02:54:44: resetting layer weights\n",
      "INFO - 02:54:44: Doc2Vec lifecycle event {'msg': 'training model with 7 workers on 11630 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=7 window=15 shrink_windows=True', 'datetime': '2022-04-30T02:54:44.353087', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no model. The model will be trained now:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:54:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:45: EPOCH - 1 : training on 294060 raw words (277822 effective words) took 0.8s, 341008 effective words/s\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:45: EPOCH - 2 : training on 294060 raw words (277696 effective words) took 0.8s, 340881 effective words/s\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:46: EPOCH - 3 : training on 294060 raw words (277824 effective words) took 0.8s, 338193 effective words/s\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:47: EPOCH - 4 : training on 294060 raw words (277881 effective words) took 0.8s, 331681 effective words/s\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:48: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:48: EPOCH - 5 : training on 294060 raw words (277797 effective words) took 0.8s, 341077 effective words/s\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:49: EPOCH - 6 : training on 294060 raw words (277859 effective words) took 0.8s, 339698 effective words/s\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:50: EPOCH - 7 : training on 294060 raw words (277962 effective words) took 0.8s, 331372 effective words/s\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:51: EPOCH - 8 : training on 294060 raw words (278031 effective words) took 0.8s, 330749 effective words/s\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:51: EPOCH - 9 : training on 294060 raw words (277939 effective words) took 0.8s, 333853 effective words/s\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:52: EPOCH - 10 : training on 294060 raw words (277917 effective words) took 0.8s, 330334 effective words/s\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:53: EPOCH - 11 : training on 294060 raw words (277884 effective words) took 0.8s, 333664 effective words/s\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:54: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:54: EPOCH - 12 : training on 294060 raw words (277979 effective words) took 0.8s, 335577 effective words/s\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 02:54:55: EPOCH - 13 : training on 294060 raw words (277760 effective words) took 0.8s, 340738 effective words/s\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:56: EPOCH - 14 : training on 294060 raw words (277857 effective words) took 0.8s, 341586 effective words/s\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 02:54:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 02:54:56: EPOCH - 15 : training on 294060 raw words (277796 effective words) took 0.8s, 340406 effective words/s\n",
      "INFO - 02:54:56: Doc2Vec lifecycle event {'msg': 'training on 4410900 raw words (4168004 effective words) took 12.5s, 333447 effective words/s', 'datetime': '2022-04-30T02:54:56.853446', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 02:54:56: Doc2Vec lifecycle event {'fname_or_handle': 'C:\\\\Users\\\\Hind\\\\Desktop\\\\GitHub code/Features/doc2vec-model.bin', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-30T02:54:56.853446', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "INFO - 02:54:56: not storing attribute cum_table\n",
      "INFO - 02:54:56: saved C:\\Users\\Hind\\Desktop\\GitHub code/Features/doc2vec-model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.21 mins\n",
      "[-0.04283398  0.00079077  0.0775322  -0.02957144  0.05562497 -0.00698017\n",
      "  0.0186095  -0.01249805 -0.03274334  0.06288268  0.023407   -0.09177446\n",
      "  0.04897289 -0.06626798 -0.10562522  0.08817029 -0.00785297  0.03037127\n",
      " -0.0091265   0.00599494  0.03655962 -0.0601322   0.1036763  -0.05080881\n",
      " -0.01161645 -0.00674384  0.06353684 -0.01104383  0.00789816  0.03166428\n",
      " -0.03410955 -0.02782872  0.03747762  0.12158677  0.04323016 -0.02921532\n",
      "  0.04462312  0.02790437 -0.03779616 -0.03516443 -0.07907736  0.10882051\n",
      " -0.00171213  0.06894293  0.08435101 -0.04271301 -0.04076056 -0.12134328\n",
      "  0.01811928 -0.02891283  0.02218465 -0.03312704 -0.0093756  -0.04591612\n",
      "  0.02166417 -0.02674035  0.02390974 -0.00349059  0.02063171 -0.04118214\n",
      "  0.01249478  0.02039344 -0.10467897  0.06346711 -0.06833743  0.04164757\n",
      "  0.10436503 -0.08953945 -0.00787543 -0.01388591 -0.0160646  -0.08648466\n",
      "  0.01331451  0.14527611  0.14969955 -0.05445182  0.00193428  0.04981213\n",
      " -0.02860853  0.07551111 -0.03063239 -0.02325559  0.04407542 -0.09489726\n",
      "  0.06603578 -0.06537752 -0.06046588  0.0879265  -0.02704341  0.02845372\n",
      "  0.07872061  0.11098618 -0.0354101   0.04210854  0.05334808  0.1000582\n",
      "  0.05396889 -0.01379878 -0.0029467  -0.01528445 -0.01811824 -0.01351606\n",
      "  0.03442931  0.07105206 -0.05990508  0.0865095  -0.03516348  0.01015009\n",
      "  0.06107953  0.04322403 -0.05093293 -0.01947857  0.00275074 -0.03401018\n",
      "  0.12993312 -0.02229998 -0.01998266  0.00586495  0.06561279  0.01170766\n",
      "  0.03175448 -0.03438901  0.00351856  0.00252588 -0.0846572   0.03867611\n",
      " -0.07311726 -0.00066437 -0.10615928  0.02339431 -0.01994467  0.09903655\n",
      "  0.01275985 -0.02191179  0.02927732 -0.00660098 -0.00048257 -0.07681402\n",
      "  0.01806801 -0.01991061 -0.00774819 -0.01541577  0.00421331  0.00368739\n",
      " -0.00331192 -0.06258038 -0.07721607 -0.01599624 -0.07355264 -0.05431776\n",
      " -0.01090717 -0.00663442  0.03774518 -0.03801936 -0.06312232 -0.00832274\n",
      " -0.07534375  0.01883242  0.00806409  0.06163139  0.01268399 -0.00914174\n",
      " -0.02937126 -0.0021784  -0.09325375  0.00351011  0.11613063 -0.09613298\n",
      "  0.03350376  0.01940675  0.05435335 -0.03604344  0.0377738   0.12402355\n",
      "  0.03575053 -0.05170586 -0.02834562 -0.03198455  0.0276033   0.06469049\n",
      "  0.02854395 -0.10585283  0.00790332 -0.07128105 -0.0394754  -0.01292754\n",
      "  0.05166249 -0.01962329 -0.10747551 -0.01471956  0.00165963 -0.07977802\n",
      "  0.03912444 -0.02486446 -0.08124959  0.02976174 -0.03823694 -0.05239443\n",
      " -0.01995404 -0.08477392 -0.04799128  0.07480245  0.01808244 -0.02914068\n",
      "  0.0573088  -0.05378509 -0.00290423  0.00432611  0.03953587  0.01460547\n",
      "  0.08682673 -0.09537582  0.09240773 -0.02590478 -0.03859228  0.00134288\n",
      "  0.02379086  0.08633024 -0.03222552 -0.03338653  0.0204235   0.03103992\n",
      " -0.02815851 -0.03031302  0.07508748  0.00437562 -0.05052449  0.04998623\n",
      "  0.04997533  0.00880689  0.07083838 -0.04308923  0.01657885 -0.00552735\n",
      " -0.02599354  0.01089856 -0.00875947  0.0906106  -0.09548945 -0.06693837\n",
      " -0.01089507  0.00531842 -0.03744322  0.02484024  0.05364026  0.02800532\n",
      "  0.06848149  0.04026947 -0.11883061 -0.15866236 -0.04074682 -0.10244494\n",
      " -0.0580307  -0.03871215 -0.02149777  0.06766788  0.05189542 -0.01802274\n",
      "  0.01427944 -0.09075474 -0.06628353 -0.04923847  0.03697848 -0.0739389\n",
      "  0.16846888 -0.0707946  -0.02161016 -0.04693267  0.01893173 -0.03997195\n",
      " -0.09332219 -0.0346906   0.01326287 -0.04055949 -0.00732504  0.07778234\n",
      " -0.13494763  0.05906308 -0.0689445   0.01409201 -0.04754059 -0.06428826\n",
      "  0.03976447 -0.06414924 -0.05688301 -0.00859185  0.06602743 -0.08459983\n",
      "  0.03040196  0.07162424 -0.14716288  0.00401311  0.05005529 -0.00088721\n",
      "  0.02278683  0.11225917 -0.03691319  0.00285651  0.02476629 -0.10097809]\n",
      "300\n",
      "Vocab size: 11630\n",
      "Trained vectors shape: (10065, 300)\n",
      "Original shape before preprocessing: (10083, 8)\n",
      "Time to clean up everything: 0.57 mins\n",
      "0    With my SO (30 years together), this kind of q...\n",
      "1    I feel like this is a sign of how young Americ...\n",
      "2    I was once interviewed as a character witness ...\n",
      "3    RJ Reynolds tobacco (Winston's, and Camel), Kr...\n",
      "4    Lol, so you didn't take the transfer that's li...\n",
      "Name: comment_text, dtype: object\n",
      "Shape after preprocessing: (10065, 10)\n",
      "Yielding data for Hateful features\n",
      "Total: 10065\n",
      "    Non toxic: 8210 (81.57% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Slightly toxic: 1189 (11.81% of total)\n",
      "\n",
      "Total: 10065\n",
      "    Highly toxic: 666 (6.62% of total)\n",
      "\n",
      "   total_length  averageWords  words  words_vs_length  capitals  \\\n",
      "0           188      4.400000     35         0.186170         4   \n",
      "1           412      4.098765     81         0.196602         8   \n",
      "2           190      4.968750     32         0.168421         8   \n",
      "3           206      4.914286     35         0.169903        10   \n",
      "4           158      5.360000     25         0.158228         1   \n",
      "\n",
      "   capitals_vs_length  capitals_vs_words  stopwords  stopwords_vs_length  \\\n",
      "0            0.021277           0.114286        118             0.627660   \n",
      "1            0.019417           0.098765        235             0.570388   \n",
      "2            0.042105           0.250000        108             0.568421   \n",
      "3            0.048544           0.285714        126             0.611650   \n",
      "4            0.006329           0.040000        101             0.639241   \n",
      "\n",
      "   stopwords_vs_words  ...  profaneRatio  nouns  adjectives  verbs  \\\n",
      "0            3.371429  ...      0.057143      6           2      6   \n",
      "1            2.901235  ...      0.086420     15           9     16   \n",
      "2            3.375000  ...      0.062500     11           0      5   \n",
      "3            3.600000  ...      0.057143     15           3      4   \n",
      "4            4.040000  ...      0.040000      5           3      6   \n",
      "\n",
      "   nouns_vs_length  adjectives_vs_length  verbs_vs_length  nouns_vs_words  \\\n",
      "0         0.031915              0.010638         0.031915        0.171429   \n",
      "1         0.036408              0.021845         0.038835        0.185185   \n",
      "2         0.057895              0.000000         0.026316        0.343750   \n",
      "3         0.072816              0.014563         0.019417        0.428571   \n",
      "4         0.031646              0.018987         0.037975        0.200000   \n",
      "\n",
      "   adjectives_vs_words  verbs_vs_words  \n",
      "0             0.057143        0.171429  \n",
      "1             0.111111        0.197531  \n",
      "2             0.000000        0.156250  \n",
      "3             0.085714        0.114286  \n",
      "4             0.120000        0.240000  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "(10065, 3000)\n",
      "(10065, 3000)\n",
      "(10065, 3000)\n",
      "(10065, 3000)\n",
      "(10065, 37)\n",
      "(10065, 300)\n",
      "(10065, 300)\n",
      "(10065, 12637)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main():\n",
    "    generateBOWFeats(1)\n",
    "    generateGensimFeats(2, 1)\n",
    "    generateGensimFeats(2, 2)\n",
    "    generateHateFeats(3)\n",
    "    combineAllFeatures()\n",
    "main()\n",
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2554efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
