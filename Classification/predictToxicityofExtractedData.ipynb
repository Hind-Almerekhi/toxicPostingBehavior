{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import gc\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "root_dir = os.path.abspath(os.curdir)\n",
    "subs_dir = os.path.dirname(root_dir)+\"/ExtractedSubmissions/\"\n",
    "coms_dir = os.path.dirname(root_dir)+\"/ExtractedComments/\"\n",
    "model_dir = os.path.dirname(root_dir)+\"/Models/pytorchmodel/\"\n",
    "#Create directory to save predictions of submissions\n",
    "pred_dir_subs = os.path.dirname(root_dir)+\"/SubmissionPredictions/\"\n",
    "os.makedirs(pred_dir_subs, exist_ok=True)\n",
    "#Create directory to save predictions of comments\n",
    "pred_dir_coms = os.path.dirname(root_dir)+\"/CommentPredictions/\"\n",
    "os.makedirs(pred_dir_coms, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 128\n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    # Divide the total iterations by the desired number of updates. Most likely\n",
    "    # this will be some ugly number.\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def clean(text, newline=True, quote=True, bullet_point=True, \n",
    "          link=True, strikethrough=True, spoiler=True,\n",
    "          code=True, superscript=True, table=True, heading=True):\n",
    "    \"\"\"\n",
    "    Cleans text (string).\n",
    "    Removes common Reddit special characters/symbols:\n",
    "      * \\n (newlines)\n",
    "      * &gt; (> quotes)\n",
    "      * * or &amp;#x200B; (bullet points)\n",
    "      * []() (links)\n",
    "      * etc (see below)\n",
    "    Specific removals can be turned off, but everything is on by default.\n",
    "    Standard punctuation etc is deliberately not removed, can be done in a\n",
    "    second round manually, or may be preserved in any case.\n",
    "    \"\"\"\n",
    "    # Newlines (replaced with space to preserve cases like word1\\nword2)\n",
    "    if newline:\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "        # Remove resulting ' '\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # > Quotes\n",
    "    if quote:\n",
    "        text = re.sub(r'\\\"?\\\\?&?gt;?', '', text)\n",
    "\n",
    "    # Bullet points/asterisk (bold/italic)\n",
    "    if bullet_point:\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # []() Link (Also removes the hyperlink)\n",
    "    if link:\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text = re.sub('~', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text = re.sub('&lt;', '', text)\n",
    "        text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Code, inline and block\n",
    "    if code:\n",
    "        text = re.sub('`', '', text)\n",
    "\n",
    "    # Superscript (Preserves the text)\n",
    "    if superscript:\n",
    "        text = re.sub(r'\\^\\((.*?)\\)', r'\\1', text)\n",
    "\n",
    "    # Table\n",
    "    if table:\n",
    "        text = re.sub(r'\\|', ' ', text)\n",
    "        text = re.sub(':-', '', text)\n",
    "\n",
    "    # Heading\n",
    "    if heading:\n",
    "        text = re.sub('#', '', text)\n",
    "    return text\n",
    "\n",
    "def make_smart_batches(text_samples, labels, batch_size):\n",
    "    '''\n",
    "    This function combines all of the required steps to prepare batches.\n",
    "    '''\n",
    "\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n",
    "\n",
    "    # =========================\n",
    "    #   Tokenize & Truncate\n",
    "    # =========================\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
    "\n",
    "    # For each training example...\n",
    "    for text in text_samples:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # Tokenize the sample.\n",
    "        input_ids = tokenizer.encode(text=text,              # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "                                    \n",
    "        # Add the tokenized result to our list.\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================    \n",
    "\n",
    "    # Sort the two lists together by the length of the input sequence.\n",
    "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
    "\n",
    "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
    "\n",
    "    import random\n",
    "\n",
    "    # List of batches that we'll construct.\n",
    "    batch_ordered_sentences = []\n",
    "    batch_ordered_labels = []\n",
    "\n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "    \n",
    "    # Loop over all of the input samples...    \n",
    "    while len(samples) > 0:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        # `to_take` is our actual batch size. It will be `batch_size` until \n",
    "        # we get to the last batch, which may be smaller. \n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # Pick a random index in the list of remaining samples to start\n",
    "        # our batch at.\n",
    "        select = random.randint(0, len(samples) - to_take)\n",
    "\n",
    "        # Select a contiguous batch of samples starting at `select`.\n",
    "        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        #print(\"Batch length:\", len(batch))\n",
    "\n",
    "        # Each sample is a tuple--split them apart to create a separate list of \n",
    "        # sequences and a list of labels for this batch.\n",
    "        batch_ordered_sentences.append([s[0] for s in batch])\n",
    "        batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "        # Remove these samples from the list.\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "    py_labels = []\n",
    "\n",
    "    # For each batch...\n",
    "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "        # New version of the batch, this time with padded sequences and now with\n",
    "        # attention masks defined.\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # First, find the longest sample in the batch. \n",
    "        # Note that the sequences do currently include the special tokens!\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # For each input in this batch...\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # How many pad tokens do we need to add?\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # Add `num_pads` padding tokens to the end of the sequence.\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # Define the attention mask--it's just a `1` for every real token\n",
    "            # and a `0` for every padding token.\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # Add the padded results to the batch.\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # Our batch has been padded, so we need to save this updated batch.\n",
    "        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
    "        # Todo - Michael's code specified \"dtype=torch.long\"\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "        py_labels.append(torch.tensor(batch_labels))\n",
    "    \n",
    "    print('  DONE.')\n",
    "\n",
    "    # Return the smart-batched dataset!\n",
    "    return (py_inputs, py_attn_masks, py_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Setting-up PyTorch...........\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Setting-up PyTorch...........\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)\n",
    "    \n",
    "def run(year,data_dir,pred_dir):\n",
    "    #year = 2006\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Loading Data...........\")\n",
    "    if data_dir == subs_dir:\n",
    "        cols = ['ID','Author','LinkID','Subreddit','Text','Time','Link']\n",
    "        data_dir = subs_dir\n",
    "        pred_dir = pred_dir_subs\n",
    "    else:\n",
    "        cols = ['ID','Author','LinkID','Subreddit','Text','Time']\n",
    "        data_dir = coms_dir\n",
    "        pred_dir = pred_dir_coms\n",
    "    data = pd.read_csv(\n",
    "        data_dir+str(year)+\".csv\",\n",
    "        header=None,\n",
    "        names=cols,\n",
    "        engine=\"python\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    data.drop(['LinkID'],\n",
    "              axis=1,\n",
    "              inplace=True)\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Data Pre-processing...........\")\n",
    "    data_clean = [clean(post) for post in data.Text.astype('str')]\n",
    "    print(\"dataset length:\",len(data_clean))\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Prepare smart batches...........\")\n",
    "    # Use our new function to completely prepare our dataset.\n",
    "    (py_inputs, py_attn_masks, py_labels) = make_smart_batches(data_clean, data.index, batch_size)\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Run prediction on data...........\")\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(data.index)))\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
    "\n",
    "    # Measure elapsed time.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(0, len(py_inputs)):\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Calculate the time remaining based on our progress.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "            remaining = format_time(remaining_sec)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "        # Copy the batch to the GPU.\n",
    "        b_input_ids = py_inputs[step].to(device)\n",
    "        b_input_mask = py_attn_masks[step].to(device)\n",
    "        b_labels = py_labels[step].to(device)\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        outputs =  F.softmax(logits, dim=1)\n",
    "        preds, labs = torch.max(outputs, 1)\n",
    "        # Move logits and labels to CPU\n",
    "        proba = outputs.detach().cpu().numpy()\n",
    "        # logits = preds.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "\n",
    "        predictions.append(proba)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    print('    DONE.')\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Preparing prediction file...........\")\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    # Choose the label with the highest score as our prediction.\n",
    "    probs = np.max(predictions, axis=1).flatten()\n",
    "    label = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    dataset = pd.DataFrame({'Index': true_labels, 'Probability':probs,'labels': label})\n",
    "    dataset['Label'] = dataset['labels'].map({0:'highly_toxic', 1:'slightly_toxic',2:'non_toxic'})\n",
    "    del dataset['labels']\n",
    "    dataset = dataset.sort_values(by=['Index'])\n",
    "    dataset = dataset.reset_index()\n",
    "    del dataset['Index']\n",
    "    dataset['ID'] = data.ID\n",
    "    dataset['Author'] = data.Author\n",
    "    dataset['Text'] = data.Text\n",
    "    dataset['Subreddit'] = data.Subreddit\n",
    "    dataset['Time'] = data.Time\n",
    "    if data_dir == subs_dir:\n",
    "        dataset['Link'] = data.Link\n",
    "        dataset = dataset[['ID','Author','Subreddit','Text','Time','Link','Probability','Label']]\n",
    "    else:\n",
    "        dataset = dataset[['ID','Author','Subreddit','Text','Time','Probability','Label']]\n",
    "    print(dataset.head())\n",
    "    out = pred_dir+str(year)+\".csv\"\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    dataset.to_csv(out,index=False)\n",
    "    del dataset\n",
    "    del data\n",
    "    del data_clean\n",
    "#     del model\n",
    "#     del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "_______________________________________________________________\n",
      "Loading Data...........\n",
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n",
      "dataset length: 10917\n",
      "_______________________________________________________________\n",
      "Prepare smart batches...........\n",
      "Creating Smart Batches from 10,917 examples with batch size 128...\n",
      "\n",
      "Tokenizing 10,917 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 1,000 samples.\n",
      "  Tokenized 2,000 samples.\n",
      "  Tokenized 3,000 samples.\n",
      "  Tokenized 4,000 samples.\n",
      "  Tokenized 5,000 samples.\n",
      "  Tokenized 6,000 samples.\n",
      "  Tokenized 7,000 samples.\n",
      "  Tokenized 8,000 samples.\n",
      "  Tokenized 9,000 samples.\n",
      "  Tokenized 10,000 samples.\n",
      "DONE.\n",
      "    10,917 samples\n",
      "\n",
      "    10,917 samples after sorting\n",
      "\n",
      "Creating batches of size 128...\n",
      "\n",
      "  DONE - Selected 86 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n",
      "_______________________________________________________________\n",
      "Run prediction on data...........\n",
      "Predicting labels for 10,917 test sentences...\n",
      "  Batch       9  of       86.    Elapsed: 0:00:01.  Remaining: 0:00:10\n",
      "  Batch      18  of       86.    Elapsed: 0:00:02.  Remaining: 0:00:07\n",
      "  Batch      27  of       86.    Elapsed: 0:00:02.  Remaining: 0:00:05\n",
      "  Batch      36  of       86.    Elapsed: 0:00:03.  Remaining: 0:00:04\n",
      "  Batch      45  of       86.    Elapsed: 0:00:04.  Remaining: 0:00:03\n",
      "  Batch      54  of       86.    Elapsed: 0:00:04.  Remaining: 0:00:03\n",
      "  Batch      63  of       86.    Elapsed: 0:00:05.  Remaining: 0:00:02\n",
      "  Batch      72  of       86.    Elapsed: 0:00:06.  Remaining: 0:00:01\n",
      "  Batch      81  of       86.    Elapsed: 0:00:07.  Remaining: 0:00:00\n",
      "    DONE.\n",
      "_______________________________________________________________\n",
      "Preparing prediction file...........\n",
      "     ID Author Subreddit                                               Text  \\\n",
      "0  fplf   jh99        de  Unionspolitiker will in Terrordatei: \"Religion...   \n",
      "1  kmfd   jh99        de  ZVS-Panne: 70 Studienbewerbern aus Versehen zu...   \n",
      "2  fpm6   jh99        de  Geistlicher will übers Wasser laufen - und ert...   \n",
      "3  2tsm   jh99        de      Sicherheitsrisiko bei neuen Chip-Kreditkarten   \n",
      "4  lb8x   jh99        de  Massengrab im Sauerland: Nichts sehen, nichts ...   \n",
      "\n",
      "                  Time                                               Link  \\\n",
      "0  2006-08-30 23:06:53  http://www.ftd.de/politik/deutschland/108468.html   \n",
      "1  2006-10-03 02:50:53  http://www.spiegel.de/unispiegel/studium/0,151...   \n",
      "2  2006-08-30 23:10:25  http://www.spiegel.de/panorama/0,1518,434184,0...   \n",
      "3  2006-03-07 21:54:16      http://www.netzeitung.de/internet/385909.html   \n",
      "4  2006-10-08 00:12:30  http://www.spiegel.de/panorama/zeitgeschichte/...   \n",
      "\n",
      "   Probability           Label  \n",
      "0     0.540501       non_toxic  \n",
      "1     0.795477       non_toxic  \n",
      "2     0.586379  slightly_toxic  \n",
      "3     0.786897       non_toxic  \n",
      "4     0.581961       non_toxic  \n",
      "_______________________________________________________________\n",
      "Loading Data...........\n",
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n",
      "dataset length: 169608\n",
      "_______________________________________________________________\n",
      "Prepare smart batches...........\n",
      "Creating Smart Batches from 169,608 examples with batch size 128...\n",
      "\n",
      "Tokenizing 169,608 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 20,000 samples.\n",
      "  Tokenized 40,000 samples.\n",
      "  Tokenized 60,000 samples.\n",
      "  Tokenized 80,000 samples.\n",
      "  Tokenized 100,000 samples.\n",
      "  Tokenized 120,000 samples.\n",
      "  Tokenized 140,000 samples.\n",
      "  Tokenized 160,000 samples.\n",
      "DONE.\n",
      "   169,608 samples\n",
      "\n",
      "   169,608 samples after sorting\n",
      "\n",
      "Creating batches of size 128...\n",
      "\n",
      "  DONE - Selected 1,326 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n",
      "_______________________________________________________________\n",
      "Run prediction on data...........\n",
      "Predicting labels for 169,608 test sentences...\n",
      "  Batch     100  of    1,326.    Elapsed: 0:00:45.  Remaining: 0:09:08\n",
      "  Batch     200  of    1,326.    Elapsed: 0:01:25.  Remaining: 0:08:00\n",
      "  Batch     300  of    1,326.    Elapsed: 0:02:15.  Remaining: 0:07:42\n",
      "  Batch     400  of    1,326.    Elapsed: 0:02:57.  Remaining: 0:06:51\n",
      "  Batch     500  of    1,326.    Elapsed: 0:03:35.  Remaining: 0:05:55\n",
      "  Batch     600  of    1,326.    Elapsed: 0:04:10.  Remaining: 0:05:03\n",
      "  Batch     700  of    1,326.    Elapsed: 0:04:48.  Remaining: 0:04:17\n",
      "  Batch     800  of    1,326.    Elapsed: 0:05:21.  Remaining: 0:03:31\n",
      "  Batch     900  of    1,326.    Elapsed: 0:06:09.  Remaining: 0:02:54\n",
      "  Batch   1,000  of    1,326.    Elapsed: 0:06:52.  Remaining: 0:02:14\n",
      "  Batch   1,100  of    1,326.    Elapsed: 0:07:21.  Remaining: 0:01:31\n",
      "  Batch   1,200  of    1,326.    Elapsed: 0:07:56.  Remaining: 0:00:50\n",
      "  Batch   1,300  of    1,326.    Elapsed: 0:08:43.  Remaining: 0:00:10\n",
      "    DONE.\n",
      "_______________________________________________________________\n",
      "Preparing prediction file...........\n",
      "      ID Author   Subreddit  \\\n",
      "0  c4o8p    0sn  reddit.com   \n",
      "1  c4odh    0sn  reddit.com   \n",
      "2  c4oqc    0sn  reddit.com   \n",
      "3  c50zx    0sn  reddit.com   \n",
      "4  c56f4    0sn  reddit.com   \n",
      "\n",
      "                                                Text                 Time  \\\n",
      "0       Shouldn't that be TenThingsIhateAboutWikis ?  2006-04-19 23:39:39   \n",
      "1  How about \"Less is not always FEWER\". for cryi...  2006-04-20 01:11:15   \n",
      "2  Yes, that's an idiomatic english phrase. Said ...  2006-04-20 05:56:50   \n",
      "3  I can't get to sleep unless I have some stimul...  2006-04-27 00:32:53   \n",
      "4  Astronomy is also one of the fastest changing ...  2006-04-30 02:47:07   \n",
      "\n",
      "   Probability      Label  \n",
      "0     0.961432  non_toxic  \n",
      "1     0.999491  non_toxic  \n",
      "2     0.999690  non_toxic  \n",
      "3     0.999562  non_toxic  \n",
      "4     0.999358  non_toxic  \n",
      "2007\n",
      "_______________________________________________________________\n",
      "Loading Data...........\n",
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n",
      "dataset length: 47556\n",
      "_______________________________________________________________\n",
      "Prepare smart batches...........\n",
      "Creating Smart Batches from 47,556 examples with batch size 128...\n",
      "\n",
      "Tokenizing 47,556 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 5,000 samples.\n",
      "  Tokenized 10,000 samples.\n",
      "  Tokenized 15,000 samples.\n",
      "  Tokenized 20,000 samples.\n",
      "  Tokenized 25,000 samples.\n",
      "  Tokenized 30,000 samples.\n",
      "  Tokenized 35,000 samples.\n",
      "  Tokenized 40,000 samples.\n",
      "  Tokenized 45,000 samples.\n",
      "DONE.\n",
      "    47,556 samples\n",
      "\n",
      "    47,556 samples after sorting\n",
      "\n",
      "Creating batches of size 128...\n",
      "\n",
      "  DONE - Selected 372 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n",
      "_______________________________________________________________\n",
      "Run prediction on data...........\n",
      "Predicting labels for 47,556 test sentences...\n",
      "  Batch      40  of      372.    Elapsed: 0:00:04.  Remaining: 0:00:33\n",
      "  Batch      80  of      372.    Elapsed: 0:00:08.  Remaining: 0:00:31\n",
      "  Batch     120  of      372.    Elapsed: 0:00:12.  Remaining: 0:00:26\n",
      "  Batch     160  of      372.    Elapsed: 0:00:15.  Remaining: 0:00:20\n",
      "  Batch     200  of      372.    Elapsed: 0:00:19.  Remaining: 0:00:16\n",
      "  Batch     240  of      372.    Elapsed: 0:00:23.  Remaining: 0:00:13\n",
      "  Batch     280  of      372.    Elapsed: 0:00:27.  Remaining: 0:00:09\n",
      "  Batch     320  of      372.    Elapsed: 0:00:30.  Remaining: 0:00:05\n",
      "  Batch     360  of      372.    Elapsed: 0:00:34.  Remaining: 0:00:01\n",
      "    DONE.\n",
      "_______________________________________________________________\n",
      "Preparing prediction file...........\n",
      "      ID         Author   Subreddit  \\\n",
      "0  60qgx  davidreiss666  reddit.com   \n",
      "1  1why7   antifolkhero  reddit.com   \n",
      "2  1mbhv         bobcat  reddit.com   \n",
      "3  2lqb2      jetsetter  reddit.com   \n",
      "4  2842l    kermityfrog  reddit.com   \n",
      "\n",
      "                                                Text                 Time  \\\n",
      "0  Are The Parents Who MySpace-Tormented Megan Me...  2007-11-16 12:04:51   \n",
      "1                    Borat Sued By Stupid New Yorker  2007-06-07 00:47:46   \n",
      "2  Hello, new Redditors. Your elder Redditors wou...  2007-05-01 03:00:04   \n",
      "3  What's the most common way for a Rockstar to D...  2007-09-04 18:24:40   \n",
      "4  What Really Happened During the Challenger Exp...  2007-07-20 19:35:18   \n",
      "\n",
      "                                                Link  Probability  \\\n",
      "0  http://jezebel.com/gossip/drew-no-blood/are-th...     0.985733   \n",
      "1  http://www.thesmokinggun.com/archive/years/200...     0.995586   \n",
      "2              http://reddit.com/info/1mbhv/comments     0.998779   \n",
      "3  http://www.economist.com/daily/chartgallery/di...     0.775700   \n",
      "4  http://www.straightdope.com/classics/a4_024b.html     0.999571   \n",
      "\n",
      "          Label  \n",
      "0     non_toxic  \n",
      "1  highly_toxic  \n",
      "2     non_toxic  \n",
      "3     non_toxic  \n",
      "4     non_toxic  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Loading Data...........\n",
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n",
      "dataset length: 849828\n",
      "_______________________________________________________________\n",
      "Prepare smart batches...........\n",
      "Creating Smart Batches from 849,828 examples with batch size 128...\n",
      "\n",
      "Tokenizing 849,828 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 80,000 samples.\n",
      "  Tokenized 160,000 samples.\n",
      "  Tokenized 240,000 samples.\n",
      "  Tokenized 320,000 samples.\n",
      "  Tokenized 400,000 samples.\n",
      "  Tokenized 480,000 samples.\n",
      "  Tokenized 560,000 samples.\n",
      "  Tokenized 640,000 samples.\n",
      "  Tokenized 720,000 samples.\n",
      "  Tokenized 800,000 samples.\n",
      "DONE.\n",
      "   849,828 samples\n",
      "\n",
      "   849,828 samples after sorting\n",
      "\n",
      "Creating batches of size 128...\n",
      "\n",
      "  DONE - Selected 6,640 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n",
      "_______________________________________________________________\n",
      "Run prediction on data...........\n",
      "Predicting labels for 849,828 test sentences...\n",
      "  Batch     700  of    6,640.    Elapsed: 0:04:01.  Remaining: 0:34:06\n",
      "  Batch   1,400  of    6,640.    Elapsed: 0:08:15.  Remaining: 0:30:53\n",
      "  Batch   2,100  of    6,640.    Elapsed: 0:12:30.  Remaining: 0:27:02\n",
      "  Batch   2,800  of    6,640.    Elapsed: 0:16:48.  Remaining: 0:23:03\n",
      "  Batch   3,500  of    6,640.    Elapsed: 0:20:35.  Remaining: 0:18:28\n",
      "  Batch   4,200  of    6,640.    Elapsed: 0:24:52.  Remaining: 0:14:27\n",
      "  Batch   4,900  of    6,640.    Elapsed: 0:28:46.  Remaining: 0:10:13\n",
      "  Batch   5,600  of    6,640.    Elapsed: 0:33:13.  Remaining: 0:06:10\n",
      "  Batch   6,300  of    6,640.    Elapsed: 0:37:29.  Remaining: 0:02:01\n",
      "    DONE.\n",
      "_______________________________________________________________\n",
      "Preparing prediction file...........\n",
      "       ID Author   Subreddit  \\\n",
      "0  c14urf    -J-  reddit.com   \n",
      "1  c14wou    -J-  reddit.com   \n",
      "2  c14uuh    -J-  reddit.com   \n",
      "3  c14y4w    -J-  reddit.com   \n",
      "4  c14ute    -J-  reddit.com   \n",
      "\n",
      "                                                Text                 Time  \\\n",
      "0  I was just reviewing my traffic records when I...  2007-02-16 06:05:28   \n",
      "1  I doubt you did read it. I have dealt with eno...  2007-02-16 15:32:33   \n",
      "2  Well I don't plan to steal. It has to be paid ...  2007-02-16 06:34:32   \n",
      "3  Clarification: I agreed to A (1) loan. Not 18 ...  2007-02-16 19:06:18   \n",
      "4  I did, however it wasn't considered \"official\"...  2007-02-16 06:21:39   \n",
      "\n",
      "   Probability      Label  \n",
      "0     0.999578  non_toxic  \n",
      "1     0.998919  non_toxic  \n",
      "2     0.999548  non_toxic  \n",
      "3     0.999592  non_toxic  \n",
      "4     0.999580  non_toxic  \n"
     ]
    }
   ],
   "source": [
    "for year in range(2006,2008):\n",
    "    print(year)\n",
    "    run(year,subs_dir,pred_dir_subs)\n",
    "    run(year,coms_dir,pred_dir_coms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
