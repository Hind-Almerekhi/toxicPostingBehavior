{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import gc\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "root_dir = os.path.abspath(os.curdir)\n",
    "subs_dir = os.path.dirname(root_dir)+\"/ExtractedSubmissions/\"\n",
    "model_dir = os.path.dirname(root_dir)+\"/Models/pytorchmodel/\"\n",
    "#Create directory to save predictions\n",
    "pred_dir = os.path.dirname(root_dir)+\"/SubmissionPredictions/\"\n",
    "os.makedirs(pred_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "batch_size = 128\n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    # Divide the total iterations by the desired number of updates. Most likely\n",
    "    # this will be some ugly number.\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def clean(text, newline=True, quote=True, bullet_point=True, \n",
    "          link=True, strikethrough=True, spoiler=True,\n",
    "          code=True, superscript=True, table=True, heading=True):\n",
    "    \"\"\"\n",
    "    Cleans text (string).\n",
    "    Removes common Reddit special characters/symbols:\n",
    "      * \\n (newlines)\n",
    "      * &gt; (> quotes)\n",
    "      * * or &amp;#x200B; (bullet points)\n",
    "      * []() (links)\n",
    "      * etc (see below)\n",
    "    Specific removals can be turned off, but everything is on by default.\n",
    "    Standard punctuation etc is deliberately not removed, can be done in a\n",
    "    second round manually, or may be preserved in any case.\n",
    "    \"\"\"\n",
    "    # Newlines (replaced with space to preserve cases like word1\\nword2)\n",
    "    if newline:\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "        # Remove resulting ' '\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # > Quotes\n",
    "    if quote:\n",
    "        text = re.sub(r'\\\"?\\\\?&?gt;?', '', text)\n",
    "\n",
    "    # Bullet points/asterisk (bold/italic)\n",
    "    if bullet_point:\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # []() Link (Also removes the hyperlink)\n",
    "    if link:\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text = re.sub('~', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text = re.sub('&lt;', '', text)\n",
    "        text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Code, inline and block\n",
    "    if code:\n",
    "        text = re.sub('`', '', text)\n",
    "\n",
    "    # Superscript (Preserves the text)\n",
    "    if superscript:\n",
    "        text = re.sub(r'\\^\\((.*?)\\)', r'\\1', text)\n",
    "\n",
    "    # Table\n",
    "    if table:\n",
    "        text = re.sub(r'\\|', ' ', text)\n",
    "        text = re.sub(':-', '', text)\n",
    "\n",
    "    # Heading\n",
    "    if heading:\n",
    "        text = re.sub('#', '', text)\n",
    "    return text\n",
    "\n",
    "def make_smart_batches(text_samples, labels, batch_size):\n",
    "    '''\n",
    "    This function combines all of the required steps to prepare batches.\n",
    "    '''\n",
    "\n",
    "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n",
    "\n",
    "    # =========================\n",
    "    #   Tokenize & Truncate\n",
    "    # =========================\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    full_input_ids = []\n",
    "\n",
    "    # Tokenize all training examples\n",
    "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
    "\n",
    "    # For each training example...\n",
    "    for text in text_samples:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
    "\n",
    "        # Tokenize the sample.\n",
    "        input_ids = tokenizer.encode(text=text,              # Text to encode.\n",
    "                                    add_special_tokens=True, # Do add specials.\n",
    "                                    max_length=max_len,      # Do Truncate!\n",
    "                                    truncation=True,         # Do Truncate!\n",
    "                                    padding=False)           # DO NOT pad.\n",
    "                                    \n",
    "        # Add the tokenized result to our list.\n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #      Select Batches\n",
    "    # =========================    \n",
    "\n",
    "    # Sort the two lists together by the length of the input sequence.\n",
    "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
    "\n",
    "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
    "\n",
    "    import random\n",
    "\n",
    "    # List of batches that we'll construct.\n",
    "    batch_ordered_sentences = []\n",
    "    batch_ordered_labels = []\n",
    "\n",
    "    print('Creating batches of size {:}...'.format(batch_size))\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "    \n",
    "    # Loop over all of the input samples...    \n",
    "    while len(samples) > 0:\n",
    "        \n",
    "        # Report progress.\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        # `to_take` is our actual batch size. It will be `batch_size` until \n",
    "        # we get to the last batch, which may be smaller. \n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # Pick a random index in the list of remaining samples to start\n",
    "        # our batch at.\n",
    "        select = random.randint(0, len(samples) - to_take)\n",
    "\n",
    "        # Select a contiguous batch of samples starting at `select`.\n",
    "        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        #print(\"Batch length:\", len(batch))\n",
    "\n",
    "        # Each sample is a tuple--split them apart to create a separate list of \n",
    "        # sequences and a list of labels for this batch.\n",
    "        batch_ordered_sentences.append([s[0] for s in batch])\n",
    "        batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "        # Remove these samples from the list.\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #        Add Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding out sequences within each batch...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "    py_labels = []\n",
    "\n",
    "    # For each batch...\n",
    "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "        # New version of the batch, this time with padded sequences and now with\n",
    "        # attention masks defined.\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # First, find the longest sample in the batch. \n",
    "        # Note that the sequences do currently include the special tokens!\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        # For each input in this batch...\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # How many pad tokens do we need to add?\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # Add `num_pads` padding tokens to the end of the sequence.\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # Define the attention mask--it's just a `1` for every real token\n",
    "            # and a `0` for every padding token.\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # Add the padded results to the batch.\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # Our batch has been padded, so we need to save this updated batch.\n",
    "        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
    "        # Todo - Michael's code specified \"dtype=torch.long\"\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "        py_labels.append(torch.tensor(batch_labels))\n",
    "    \n",
    "    print('  DONE.')\n",
    "\n",
    "    # Return the smart-batched dataset!\n",
    "    return (py_inputs, py_attn_masks, py_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Setting-up PyTorch...........\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Setting-up PyTorch...........\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)\n",
    "    \n",
    "def run(year):\n",
    "    #year = 2006\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Loading Data...........\")\n",
    "    cols = ['ID','Author','LinkID','Subreddit','Text','Time','Link']\n",
    "    data = pd.read_csv(\n",
    "        subs_dir+str(year)+\".csv\",\n",
    "        header=None,\n",
    "        names=cols,\n",
    "        engine=\"python\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    data.drop(['LinkID'],\n",
    "              axis=1,\n",
    "              inplace=True)\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Data Pre-processing...........\")\n",
    "    data_clean = [clean(post) for post in data.Text.astype('str')]\n",
    "    print(\"dataset length:\",len(data_clean))\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Prepare smart batches...........\")\n",
    "    # Use our new function to completely prepare our dataset.\n",
    "    (py_inputs, py_attn_masks, py_labels) = make_smart_batches(data_clean, data.index, batch_size)\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Run prediction on data...........\")\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(data.index)))\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Choose an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
    "\n",
    "    # Measure elapsed time.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(0, len(py_inputs)):\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Calculate the time remaining based on our progress.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
    "            remaining = format_time(remaining_sec)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
    "\n",
    "        # Copy the batch to the GPU.\n",
    "        b_input_ids = py_inputs[step].to(device)\n",
    "        b_input_mask = py_attn_masks[step].to(device)\n",
    "        b_labels = py_labels[step].to(device)\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        outputs =  F.softmax(logits, dim=1)\n",
    "        preds, labs = torch.max(outputs, 1)\n",
    "        # Move logits and labels to CPU\n",
    "        proba = outputs.detach().cpu().numpy()\n",
    "        # logits = preds.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "\n",
    "        predictions.append(proba)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    print('    DONE.')\n",
    "    print(\"_______________________________________________________________\")\n",
    "    print(\"Preparing prediction file...........\")\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    # Choose the label with the highest score as our prediction.\n",
    "    probs = np.max(predictions, axis=1).flatten()\n",
    "    label = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    dataset = pd.DataFrame({'Index': true_labels, 'Probability':probs,'labels': label})\n",
    "    dataset['Label'] = dataset['labels'].map({0:'highly_toxic', 1:'slightly_toxic',2:'non_toxic'})\n",
    "    del dataset['labels']\n",
    "    dataset = dataset.sort_values(by=['Index'])\n",
    "    dataset = dataset.reset_index()\n",
    "    del dataset['Index']\n",
    "    dataset['ID'] = data.ID\n",
    "    dataset['Author'] = data.Author\n",
    "    dataset['Text'] = data.Text\n",
    "    dataset['Subreddit'] = data.Subreddit\n",
    "    dataset['Time'] = data.Time\n",
    "    dataset['Link'] = data.Link\n",
    "    dataset = dataset[['ID','Author','Subreddit','Text','Time','Link','Probability','Label']]\n",
    "    print(dataset.head())\n",
    "    out = pred_dir+str(year)+\".csv\"\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    dataset.to_csv(out,index=False)\n",
    "    del dataset\n",
    "    del data\n",
    "    del data_clean\n",
    "#     del model\n",
    "#     del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "_______________________________________________________________\n",
      "Loading Data...........\n",
      "_______________________________________________________________\n",
      "Data Pre-processing...........\n",
      "dataset length: 10917\n",
      "_______________________________________________________________\n",
      "Prepare smart batches...........\n",
      "Creating Smart Batches from 10,917 examples with batch size 128...\n",
      "\n",
      "Tokenizing 10,917 samples...\n",
      "  Tokenized 0 samples.\n",
      "  Tokenized 1,000 samples.\n",
      "  Tokenized 2,000 samples.\n",
      "  Tokenized 3,000 samples.\n",
      "  Tokenized 4,000 samples.\n",
      "  Tokenized 5,000 samples.\n",
      "  Tokenized 6,000 samples.\n",
      "  Tokenized 7,000 samples.\n",
      "  Tokenized 8,000 samples.\n",
      "  Tokenized 9,000 samples.\n",
      "  Tokenized 10,000 samples.\n",
      "DONE.\n",
      "    10,917 samples\n",
      "\n",
      "    10,917 samples after sorting\n",
      "\n",
      "Creating batches of size 128...\n",
      "\n",
      "  DONE - Selected 86 batches.\n",
      "\n",
      "Padding out sequences within each batch...\n",
      "  DONE.\n",
      "_______________________________________________________________\n",
      "Run prediction on data...........\n",
      "Predicting labels for 10,917 test sentences...\n",
      "  Batch       9  of       86.    Elapsed: 0:00:01.  Remaining: 0:00:08\n",
      "  Batch      18  of       86.    Elapsed: 0:00:02.  Remaining: 0:00:06\n",
      "  Batch      27  of       86.    Elapsed: 0:00:02.  Remaining: 0:00:05\n",
      "  Batch      36  of       86.    Elapsed: 0:00:03.  Remaining: 0:00:04\n",
      "  Batch      45  of       86.    Elapsed: 0:00:03.  Remaining: 0:00:03\n"
     ]
    }
   ],
   "source": [
    "for year in range(2006,2008):\n",
    "    print(year)\n",
    "    run(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
